## Data Cleaning / Processing for ML

# Splitting training and testing data using Scikit-Learn (80/20 split)

from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(DataFrame, test_size=0.2, random_state=42) # Pick any randomstate

# Stratified Sampling ("It is important to havea sufficient number of instances in your dataset for each stratum, or else the estimateof the stratumâ€™s importance may be biased. This means that you should not have toomany strata, and each stratum should be large enough", HoML- 2nd Edition)

from sklearn.model_selection import StratifiedShuffleSplit

for train_index, test_index in split.split(housing, DataFrame["Feature"]):      # Determine on which feature you want to do Stratified Sampling
strat_train_set = housing.loc[train_index]
strat_test_set = housing.loc[test_index]

# Calculating the amount of NA values per column, and dropping the NA Values

print(df.isna().sum())
NAColumn_Names = DataFrame.columns[df.isna().any()].tolist()

df.dropna(subset=["Column Names"]) # Dropping a few columns based on their name 
df.dropna(subset=NAColumn_Names)   # Dropping all columns that contain NA Values

# Replacing missing values with median, mean, etc (using fillna) 

df["Column Names"].fillna(median, inplace=True)

# Replacing missing values with median, mean, etc (using Scikit-Learn's Imputer) only works on numerical features (drop categorical)

from sklearn.preprocessing import Imputer
imputer = Imputer(strategy="median")     

categorical = df.select_dtypes(include=['category', object]).columns
df_numerical = df.drop(categorical,axis=1) # Dropping any categorical variables

imputer.fit(df_numerical) # Computing the median of each numerical attribute
imputer.statistics_ # Checking the median values to ensure that everthing is working properly

X = imputer.transform(df_numerical) # Transforms the missing values into their median column values NumPy Array 

Training_Set = pd.DataFrame(X, columns=df_numerical.columns)

# Handling Categorical and Text Attributes Using Scikit-Learn's OneHotEncoder

from sklearn.preprocessing import OneHotEncoder
cat_encoder = OneHotEncoder()

cat_1hot = cat_encoder.fit_transform(categorical)  # 1hot encoding the categorical varibles. Creates a SciPy sparse matrix. convert to NumPy array using toarray()

# Feature Scaling, Min-Max and Normalization (z)

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

MinMax = MinMaxScaler()
ZScaling = StandardScaler()

MinMaxScaling = MinMax.fit_transform(df_numerical)
ZNormalization = ZScaling.fit_transform(df_numerical)

# Transformation Pipeline using Pipeline and ColumnTransformer (bring the numerical transformations together in one sequence

from sklearn.pipeline import Pipeline

Numerical_Pipeline = Pipeline([
    ('imputer', Imputer(strategy="median")),
    ('std_scaler', StandardScaler())
     ])
     
df_numerical_transform = Numerical_pipeline.fit_transform(df_numerical) # doing the numerical transformations at once

from sklearn.compose import ColumnTransformer

num_attribs = list(df_numerical)
cat_attribs = list(categorical)

full_pipeline = ColumnTransformer([ 
       ("num", Numerical_Pipeline, num_attribs),
       ("cat", OneHotEncoder(), cat_attribs),
    ])
    
data_prepared = full_pipeline.fit_transform(df)


